{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648c3a63-cf36-486d-a1e0-c225d1f46925",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 一般設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30644ae-8d68-444d-94d0-19b4ae2670f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
    "from statsmodels.discrete.discrete_model import Poisson as PoisDM\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning, HessianInversionWarning\n",
    "from sklearn.linear_model import PoissonRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c9343-df60-4787-8c1e-a955cd7bfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定\n",
    "FUTURE_SRC_DIR = Path(\"processed_tmp/future\") # 学習用のデータが保存されているディレクトリを設定\n",
    "ENGINE = \"fastparquet\" # 読み込み方法\n",
    "DATE_COL = \"sold_date\"\n",
    "BAN_COLS_COMMON = {DATE_COL}\n",
    "BAN_PREFIXES = []\n",
    "\n",
    "TARGET_BASE = \"future_sales\"\n",
    "H_TARGET_FMT = TARGET_BASE + \"_{}\"\n",
    "SUM_TARGET = \"future_sales_sum\"\n",
    "\n",
    "K_CANDIDATES = list(range(60, 301, 60)) #選択する特徴量の数のリストを作成\n",
    "NB_TOP = 60 #NBまたはポアソンモデルの回帰に使用する特徴量数の設定\n",
    "\n",
    "LGB_NUM_BOOST_ROUND = 8000 #LightGBMの最大学習エポック数\n",
    "LGB_EARLY_STOP_ROUNDS = 200 #LightGBMのEarly Stoppingのエポック数\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "# カテゴリ変数を設定\n",
    "CAT_INT_COLS = [\"jan_code\", \"store_name\", \"中分類名\", \"price_category\"]\n",
    "\n",
    "# 外れ値を処理する上端下端の設定\n",
    "WINSOR_LO_Q = 0.5 / 100\n",
    "WINSOR_HI_Q = 99.5 / 100\n",
    "# arcsinhのスケーリングの基準値となる値の位置の設定\n",
    "ASINH_Q = 0.90\n",
    "\n",
    "# --- 出力ディレクトリ設定 ---\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ROOT_OUT_DIR = Path(f\"lgb_nb_stacking_kgrid_all_{RUN_TAG}\")\n",
    "ROOT_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EVAL_OUT = ROOT_OUT_DIR / \"eval_results\"\n",
    "EVAL_OUT.mkdir(parents=True, exist_ok=True)\n",
    "EVAL_CSV_PATH = EVAL_OUT / \"metrics_all_tasks.csv\"\n",
    "EVAL_JSON_PATH = EVAL_OUT / \"metrics_summary.json\"\n",
    "\n",
    "print(f\"出力ディレクトリが作成されました: {ROOT_OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08ab32-0e19-4273-8e78-ad297daa9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj: Any, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_src_df(task: str) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"タスク別データを読み込み（目的変数NaNのみ除外、他は保持）\"\"\"\n",
    "    if task == \"sum\":\n",
    "        src_path = FUTURE_SRC_DIR / \"future_sum26.parquet\"\n",
    "        target = SUM_TARGET\n",
    "    else:\n",
    "        N = int(task[1:])\n",
    "        src_path = FUTURE_SRC_DIR / f\"future_h{N}.parquet\"\n",
    "        target = H_TARGET_FMT.format(N)\n",
    "\n",
    "    print(f\"      データ読み込み: {src_path}\")\n",
    "    if not src_path.exists():\n",
    "        raise FileNotFoundError(f\"{src_path} が見つかりません。\")\n",
    "\n",
    "    df = pd.read_parquet(src_path, engine=ENGINE)\n",
    "    print(f\"      - 完了: {len(df):,} 行 × {len(df.columns)} 列\")\n",
    "    \n",
    "    if DATE_COL not in df.columns:\n",
    "        raise KeyError(f\"{DATE_COL} が見つかりません。\")\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "\n",
    "    if target not in df.columns:\n",
    "        raise KeyError(f\"目的変数 {target} が見つかりません。\")\n",
    "    print(f\"      - 目的変数: {target}\")\n",
    "    \n",
    "    # 目的変数列のみNaNを除外（他の欠損は保持）\n",
    "    before_len = len(df)\n",
    "    df = df.dropna(subset=[target]).copy()\n",
    "    after_len = len(df)\n",
    "    if before_len != after_len:\n",
    "        print(f\"      - 目的変数NaN除去: {before_len:,} → {after_len:,} 行\")\n",
    "\n",
    "    # 目的変数を非負整数にクリップ\n",
    "    df[target] = np.maximum(pd.to_numeric(df[target], errors=\"coerce\").fillna(0).astype(\"Int64\"), 0).astype(np.int64)\n",
    "    \n",
    "    # 時系列順に整列\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "    print(f\"      - 時系列ソート完了: {df[DATE_COL].min()} ～ {df[DATE_COL].max()}\")\n",
    "    return df, target\n",
    "\n",
    "def time_split_by_date(df: pd.DataFrame, train_ratio: float = 0.8, date_col: str = DATE_COL) -> Tuple[pd.Timestamp, np.ndarray, np.ndarray]:\n",
    "    \"\"\"時系列8:2分割（同一日付を跨がない厳密分割）\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return pd.NaT, np.array([], dtype=int), np.array([], dtype=int)\n",
    "\n",
    "    # すでに時系列順にソート済みを想定\n",
    "    df_sorted = df.sort_values(date_col)\n",
    "    n = len(df_sorted)\n",
    "    \n",
    "    # 行ベースで80%の位置を決める\n",
    "    cut_row = max(1, int(n * train_ratio)) - 1  # 0-index\n",
    "    raw_cutoff = df_sorted.iloc[cut_row][date_col]\n",
    "    cutoff_date = pd.to_datetime(raw_cutoff)\n",
    "    \n",
    "    print(f\"       - 基準日決定: {cutoff_date} (行数ベース80%位置)\")\n",
    "\n",
    "    # 同一日付を跨がないよう <= / > で分割\n",
    "    train_mask = df[date_col] <= cutoff_date\n",
    "    val_mask = df[date_col] > cutoff_date\n",
    "    \n",
    "    print(f\"        - 初期分割: train={train_mask.sum():,} 行, val={val_mask.sum():,} 行\")\n",
    "\n",
    "    # valが空の場合の対処\n",
    "    if val_mask.sum() == 0:\n",
    "        print(f\"        - valが空のため、直前ユニーク日に切り下げ\")\n",
    "        uniq_dates = np.sort(df[date_col].unique())\n",
    "        pos = np.searchsorted(uniq_dates, cutoff_date, side=\"left\")\n",
    "        if pos > 0:\n",
    "            cutoff_date = uniq_dates[pos - 1]\n",
    "            train_mask = df[date_col] <= cutoff_date\n",
    "            val_mask = df[date_col] > cutoff_date\n",
    "            print(f\"        - 切り下げ後: train={train_mask.sum():,} 行, val={val_mask.sum():,} 行\")\n",
    "        \n",
    "        # それでもvalが空なら最終日をvalに\n",
    "        if val_mask.sum() == 0:\n",
    "            print(f\"        - 最終日をvalに設定\")\n",
    "            last_date = uniq_dates[-1]\n",
    "            train_mask = df[date_col] < last_date\n",
    "            val_mask = df[date_col] >= last_date\n",
    "            cutoff_date = last_date\n",
    "            print(f\"        - 最終分割: train={train_mask.sum():,} 行, val={val_mask.sum():,} 行\")\n",
    "\n",
    "    tr_idx = df.index[train_mask].to_numpy()\n",
    "    va_idx = df.index[val_mask].to_numpy()\n",
    "    \n",
    "    # 同一日付がtrain/valに跨っていないことを確認\n",
    "    train_dates = set(df.loc[train_mask, date_col].unique())\n",
    "    val_dates = set(df.loc[val_mask, date_col].unique())\n",
    "    overlap = train_dates & val_dates\n",
    "    if overlap:\n",
    "        raise ValueError(f\"同一日付がtrain/valに跨っています: {overlap}\")\n",
    "    \n",
    "    print(f\"        - 分割完了: cutoff_date={cutoff_date}, train={len(tr_idx):,} 行, val={len(va_idx):,} 行\")\n",
    "    return cutoff_date, tr_idx, va_idx\n",
    "\n",
    "def pick_feature_cols(df: pd.DataFrame, target_col: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"特徴列を抽出\"\"\"\n",
    "    cols = []\n",
    "    ban_cols = set(BAN_COLS_COMMON) | {target_col}\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if c in ban_cols:\n",
    "            continue\n",
    "        # 将来リークの可能性があるprefixを除外\n",
    "        if any(c.startswith(p) for p in BAN_PREFIXES):\n",
    "            continue\n",
    "        # 日時列を除外\n",
    "        if np.issubdtype(df[c].dtype, np.datetime64):\n",
    "            continue\n",
    "        cols.append(c)\n",
    "    \n",
    "    # 数値列（カテゴリ整数列を除外）\n",
    "    num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c]) and c not in CAT_INT_COLS]\n",
    "    # カテゴリ列（定義済みのカテゴリ整数列のみ）\n",
    "    cat_cols = [c for c in CAT_INT_COLS if c in df.columns]\n",
    "    \n",
    "    print(f\"        - 特徴量選択: 数値={len(num_cols)}, カテゴリ={len(cat_cols)}, 除外={len(df.columns) - len(num_cols) - len(cat_cols) - len(ban_cols)}\")\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def make_3fold_80_20_indices(df: pd.DataFrame, date_col: str = DATE_COL) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"3-fold Time Series Split（sold_dateベースで3等分、各fold内で8:2分割）\"\"\"\n",
    "    # 全データのsold_dateを取得してソート\n",
    "    df_sorted = df.sort_values(date_col).reset_index(drop=True)\n",
    "    unique_dates = df_sorted[date_col].unique()\n",
    "    n_dates = len(unique_dates)\n",
    "    \n",
    "    print(f\"        - ユニーク日付数: {n_dates:,} 日\")\n",
    "    \n",
    "    # 3等分の基準日を決定\n",
    "    date_1_3 = unique_dates[n_dates // 3]\n",
    "    date_2_3 = unique_dates[2 * n_dates // 3]\n",
    "    \n",
    "    print(f\"        - 3等分基準日: {date_1_3}, {date_2_3}\")\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for k in range(3):\n",
    "        # 各foldの日付範囲を決定\n",
    "        if k == 0:\n",
    "            # Fold 1: 最初の1/3\n",
    "            fold_start_date = unique_dates[0]\n",
    "            fold_end_date = date_1_3\n",
    "        elif k == 1:\n",
    "            # Fold 2: 中間の1/3\n",
    "            fold_start_date = date_1_3\n",
    "            fold_end_date = date_2_3\n",
    "        else:\n",
    "            # Fold 3: 最後の1/3\n",
    "            fold_start_date = date_2_3\n",
    "            fold_end_date = unique_dates[-1]\n",
    "        \n",
    "        # 各fold内のデータを取得\n",
    "        fold_mask = (df_sorted[date_col] >= fold_start_date) & (df_sorted[date_col] <= fold_end_date)\n",
    "        fold_df = df_sorted[fold_mask].reset_index(drop=True)\n",
    "        fold_dates = fold_df[date_col].unique()\n",
    "        n_fold_dates = len(fold_dates)\n",
    "        \n",
    "        # fold内で8:2分割の基準日を決定\n",
    "        train_end_date_idx = int(n_fold_dates * 0.8)\n",
    "        train_end_date = fold_dates[train_end_date_idx]\n",
    "        \n",
    "        # train/valのインデックスを取得\n",
    "        train_mask = fold_df[date_col] <= train_end_date\n",
    "        val_mask = fold_df[date_col] > train_end_date\n",
    "        \n",
    "        # 元のDataFrameのインデックスに変換\n",
    "        train_idx = fold_df[train_mask].index.to_numpy()\n",
    "        val_idx = fold_df[val_mask].index.to_numpy()\n",
    "        \n",
    "        print(f\"        - Fold {k+1}: 日付範囲[{fold_start_date}～{fold_end_date}]\")\n",
    "        print(f\"          - train: {len(train_idx):,} 行 (～{train_end_date}), val: {len(val_idx):,} 行\")\n",
    "        \n",
    "        folds.append((train_idx, val_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65571d-4b5a-435d-b5e9-90747dfec0e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LightGBM学習用の汎用関数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e3367-7d3a-43f8-99d6-647103c60cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_preprocess_for_lgb(df: pd.DataFrame, num_cols: List[str], cat_cols: List[str]) -> pd.DataFrame:\n",
    "    X = df[num_cols + cat_cols].copy()\n",
    "    for c in num_cols:\n",
    "        X[c] = X[c].astype(float).fillna(0.0)\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].astype(\"Int64\").fillna(-1)\n",
    "    return X\n",
    "\n",
    "def lgb_train_eval(\n",
    "    X_tr: pd.DataFrame, y_tr: np.ndarray,\n",
    "    X_va: Optional[pd.DataFrame], y_va: Optional[np.ndarray],\n",
    "    cat_cols: List[str],\n",
    "    num_boost_round: int = LGB_NUM_BOOST_ROUND,\n",
    "    early_stopping_rounds: int = LGB_EARLY_STOP_ROUNDS\n",
    ") -> Tuple[lgb.Booster, np.ndarray, float]:\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=True)\n",
    "    valid_sets = [dtrain]; valid_names = [\"train\"]\n",
    "    callbacks = []\n",
    "    dvalid = None\n",
    "    if X_va is not None and y_va is not None:\n",
    "        dvalid = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, reference=dtrain, free_raw_data=True)\n",
    "        valid_sets = [dtrain, dvalid]; valid_names = [\"train\", \"valid\"]\n",
    "        callbacks = [lgb.early_stopping(early_stopping_rounds, verbose=False)]\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"tweedie\", tweedie_variance_power=1.5,\n",
    "        learning_rate=0.05, num_leaves=64, min_data_in_leaf=100,\n",
    "        feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
    "        lambda_l1=0.0, max_depth=-1, seed=RANDOM_SEED, verbose=-1,\n",
    "        metric=\"rmse\", first_metric_only=True, feature_pre_filter=False,\n",
    "    )\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params=params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=valid_sets,\n",
    "        valid_names=valid_names,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    del dtrain\n",
    "    if dvalid is not None:\n",
    "        del dvalid\n",
    "    gc.collect()\n",
    "\n",
    "    if X_va is None or y_va is None:\n",
    "        return booster, np.array([]), np.nan\n",
    "\n",
    "    pred_va = booster.predict(X_va, num_iteration=booster.best_iteration)\n",
    "    rmse = float(np.sqrt(np.mean((y_va - pred_va) ** 2)))\n",
    "    return booster, pred_va, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6e75d-091e-40fc-8d55-b3f5160df631",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LightGBMの第1・2ステップ学習\n",
    "\n",
    "- Step 1 : まずは全データを3つに分割したもので全ての特徴量を用いて学習を行い、**特徴量の重要度ランキング**を作成する\n",
    "- Step 2 : 特徴量重要度ランキングを用いて、**Top K**個の特徴量を用いて学習した時の精度を評価して、最適な特徴量数を選択する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339e7a0e-647c-47ec-aa77-60a3a139c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_fold_importances_raw(\n",
    "    df_train: pd.DataFrame, target_col: str,\n",
    "    num_cols: List[str], cat_cols: List[str],\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]],\n",
    "    out_dir: Path\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"trainデータのみで特徴重要度計算（時系列リーク防止）\"\"\"\n",
    "    print(f\"    LightGBM特徴重要度計算開始 (trainのみ, {len(folds)} folds)\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    all_imps = []\n",
    "    cols = num_cols + cat_cols\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "    print(f\"    - 総特徴量数: {len(cols)} (数値:{len(num_cols)}, カテゴリ:{len(cat_cols)})\")\n",
    "    print(f\"    - trainデータ: {len(df_train):,} 行\")\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, start=1):\n",
    "        print(f\"      Fold {i}/{len(folds)} 処理中...\")\n",
    "        fdir = out_dir / f\"fold{i}\"\n",
    "        fdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # train内でのfold分割\n",
    "        X_tr = lgb_preprocess_for_lgb(df_train.iloc[tr_idx], num_cols, cat_cols)\n",
    "        X_va = lgb_preprocess_for_lgb(df_train.iloc[va_idx], num_cols, cat_cols)\n",
    "        y_tr = y_train[tr_idx]; y_va = y_train[va_idx]\n",
    "        print(f\"        - 学習データ: {len(X_tr):,} 行\")\n",
    "        print(f\"        - 検証データ: {len(X_va):,} 行\")\n",
    "\n",
    "        booster, pred_va, rmse = lgb_train_eval(X_tr, y_tr, X_va, y_va, cat_cols)\n",
    "        save_pickle(booster, fdir / \"lgb_importance_model.pkl\")\n",
    "        \n",
    "        # R²計算\n",
    "        ss_res = np.sum((y_va - pred_va) ** 2)\n",
    "        ss_tot = np.sum((y_va - np.mean(y_va)) ** 2)\n",
    "        r2 = 1.0 - (ss_res / (ss_tot + 1e-9))\n",
    "        print(f\"        - 学習完了: RMSE={rmse:.6f}, R²={r2:.6f}\")\n",
    "\n",
    "        imp = pd.DataFrame({\n",
    "            \"feature\": cols,\n",
    "            \"gain\": booster.feature_importance(importance_type=\"gain\"),\n",
    "            \"split\": booster.feature_importance(importance_type=\"split\"),\n",
    "            \"fold\": i,\n",
    "            \"rmse_val\": rmse\n",
    "        })\n",
    "        imp.sort_values([\"gain\", \"split\"], ascending=False).to_csv(fdir / \"feature_importance.csv\", index=False)\n",
    "        all_imps.append(imp)\n",
    "\n",
    "        del booster, X_tr, X_va, y_tr, y_va, imp\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"    重要度集約中...\")\n",
    "    imp_cat = pd.concat(all_imps, axis=0, ignore_index=True)\n",
    "    avg = (imp_cat.groupby(\"feature\", as_index=False)\n",
    "                 .agg(gain_mean=(\"gain\",\"mean\"), split_mean=(\"split\",\"mean\")))\n",
    "    avg = avg.sort_values([\"gain_mean\", \"split_mean\"], ascending=False).reset_index(drop=True)\n",
    "    avg.to_csv(out_dir / \"feature_importance_avg.csv\", index=False)\n",
    "    print(f\"    - 重要度集約完了: {len(avg)} 特徴量\")\n",
    "\n",
    "    del all_imps, imp_cat\n",
    "    gc.collect()\n",
    "    return avg\n",
    "\n",
    "def lgb_kgrid_cv_raw(\n",
    "    df_train: pd.DataFrame, target_col: str,\n",
    "    num_cols: List[str], cat_cols: List[str],\n",
    "    avg_importance: pd.DataFrame,\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]],\n",
    "    k_list: List[int], out_dir: Path\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"trainデータのみでKグリッドCV（時系列リーク防止）\"\"\"\n",
    "    print(f\"    KグリッドCV開始: K候補={k_list} (trainのみ)\")\n",
    "    rows = []\n",
    "    rank = avg_importance[\"feature\"].tolist()\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "    print(f\"    - trainデータ: {len(df_train):,} 行\")\n",
    "\n",
    "    for K in k_list:\n",
    "        print(f\"      K={K} 処理中...\")\n",
    "        kdir = out_dir / f\"K{K}\"\n",
    "        kdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        topk = rank[:K]\n",
    "        sel_num = [c for c in topk if c in num_cols]\n",
    "        sel_cat = [c for c in topk if c in cat_cols]\n",
    "        print(f\"        - 選択特徴量: {len(sel_num)} 数値 + {len(sel_cat)} カテゴリ = {len(topk)} 総数\")\n",
    "\n",
    "        fold_metrics = []\n",
    "        for i, (tr_idx, va_idx) in enumerate(folds, start=1):\n",
    "            fdir = kdir / f\"fold{i}\"\n",
    "            fdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # train内でのfold分割\n",
    "            X_tr = lgb_preprocess_for_lgb(df_train.iloc[tr_idx], sel_num, sel_cat)\n",
    "            y_tr = y_train[tr_idx]\n",
    "            X_va = lgb_preprocess_for_lgb(df_train.iloc[va_idx], sel_num, sel_cat)\n",
    "            y_va = y_train[va_idx]\n",
    "\n",
    "            booster, pred_va, rmse = lgb_train_eval(X_tr, y_tr, X_va, y_va, sel_cat)\n",
    "            save_pickle(booster, fdir / \"lgb_model.pkl\")\n",
    "            pd.DataFrame({\"y\": y_va, \"pred\": pred_va}).to_csv(fdir / \"pred_val.csv\", index=False)\n",
    "\n",
    "            # R²計算\n",
    "            ss_res = np.sum((y_va - pred_va) ** 2)\n",
    "            ss_tot = np.sum((y_va - np.mean(y_va)) ** 2)\n",
    "            r2 = 1.0 - (ss_res / (ss_tot + 1e-9))\n",
    "            \n",
    "            fold_metrics.append({\"fold\": i, \"RMSE_va\": rmse, \"R2_va\": r2})\n",
    "\n",
    "            del booster, X_tr, X_va, y_tr, y_va, pred_va\n",
    "            gc.collect()\n",
    "\n",
    "        dfm = pd.DataFrame(fold_metrics)\n",
    "        dfm.to_csv(kdir / \"fold_metrics.csv\", index=False)\n",
    "        rmse_mean = float(dfm[\"RMSE_va\"].mean())\n",
    "        rmse_std = float(dfm[\"RMSE_va\"].std(ddof=0))\n",
    "        r2_mean = float(dfm[\"R2_va\"].mean())\n",
    "        r2_std = float(dfm[\"R2_va\"].std(ddof=0))\n",
    "        row = {\"K\": K, \"RMSE_va_mean\": rmse_mean, \"RMSE_va_std\": rmse_std, \"R2_va_mean\": r2_mean, \"R2_va_std\": r2_std}\n",
    "        rows.append(row)\n",
    "        print(f\"        - RMSE: {rmse_mean:.6f} ± {rmse_std:.6f}, R²: {r2_mean:.6f} ± {r2_std:.6f}\")\n",
    "\n",
    "        del fold_metrics, dfm, topk, sel_num, sel_cat\n",
    "        gc.collect()\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values(\"RMSE_va_mean\").reset_index(drop=True)\n",
    "    res.to_csv(out_dir / \"kgrid_cv_results.csv\", index=False)\n",
    "    print(f\"    KグリッドCV完了: 最良K={res.iloc[0]['K']}, RMSE={res.iloc[0]['RMSE_va_mean']:.6f}, R²={res.iloc[0]['R2_va_mean']:.6f}\")\n",
    "    del rows, rank, y_train\n",
    "    gc.collect()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21739a91-88b4-4486-b917-ca9903e644e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Poisson回帰またはNegative Binomial回帰の設定(目的変数の持つ分布に応じて選択し回帰する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de499407-d523-48ee-b0ba-8f367ff22623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize(train: pd.DataFrame, valid: pd.DataFrame, cols: List[str],\n",
    "              lo_q=WINSOR_LO_Q, hi_q=WINSOR_HI_Q):\n",
    "    tr = train.copy(); va = valid.copy()\n",
    "    bounds = {}\n",
    "    for c in cols:\n",
    "        tr[c] = tr[c].astype(float)\n",
    "        va[c] = va[c].astype(float)\n",
    "        lo = np.nanquantile(tr[c], lo_q); hi = np.nanquantile(tr[c], hi_q)\n",
    "        tr[c] = np.clip(tr[c], lo, hi)\n",
    "        va[c] = np.clip(va[c], lo, hi)\n",
    "        bounds[c] = {\"lo\": float(lo), \"hi\": float(hi)}\n",
    "    return tr, va, bounds\n",
    "\n",
    "def asinh_scale(train: pd.DataFrame, valid: pd.DataFrame, cols: List[str], q=ASINH_Q):\n",
    "    tr = train.copy(); va = valid.copy(); scalers = {}\n",
    "    for c in cols:\n",
    "        base = np.asarray(tr[c].values, float)\n",
    "        cval = np.quantile(np.abs(base[~np.isnan(base)]), q) if base.size else 1.0\n",
    "        if not np.isfinite(cval) or cval <= 0: cval = 1.0\n",
    "        tr[c] = np.arcsinh(np.asarray(tr[c].values, float) / cval)\n",
    "        va[c] = np.arcsinh(np.asarray(va[c].values, float) / cval)\n",
    "        scalers[c] = float(cval)\n",
    "    return tr, va, scalers\n",
    "\n",
    "def improved_asinh_scale(train: pd.DataFrame, valid: pd.DataFrame, cols: List[str]):\n",
    "    \"\"\"NB用の改良asinhスケーリング（trainデータから学習）\"\"\"\n",
    "    tr = train.copy(); va = valid.copy(); scalers = {}\n",
    "    for c in cols:\n",
    "        base = np.asarray(tr[c].values, float)\n",
    "        \n",
    "        # trainデータから統計量を学習\n",
    "        mean_val = np.nanmean(base)\n",
    "        std_val = np.nanstd(base)\n",
    "        q95 = np.nanquantile(np.abs(base), 0.95)\n",
    "        \n",
    "        # より適切なスケーリングパラメータを計算\n",
    "        if std_val > 0:\n",
    "            # 標準化 + asinh変換\n",
    "            scale_factor = max(std_val, q95 / 2.0)  # stdと95%分位点の大きい方\n",
    "        else:\n",
    "            scale_factor = max(q95, 1.0)\n",
    "        \n",
    "        # asinh変換を適用\n",
    "        tr[c] = np.arcsinh((base - mean_val) / scale_factor)\n",
    "        va[c] = np.arcsinh((np.asarray(va[c].values, float) - mean_val) / scale_factor)\n",
    "        \n",
    "        scalers[c] = {\n",
    "            \"mean\": float(mean_val),\n",
    "            \"std\": float(std_val),\n",
    "            \"scale_factor\": float(scale_factor),\n",
    "            \"q95\": float(q95)\n",
    "        }\n",
    "    return tr, va, scalers\n",
    "\n",
    "def prepare_int_cats_for_ohe(\n",
    "    train: pd.DataFrame, valid: pd.DataFrame, cat_int_cols: List[str], max_levels: int = 30\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, List[Any]]]:\n",
    "    tr = train.copy(); va = valid.copy(); kept: Dict[str, List[Any]] = {}\n",
    "    for c in cat_int_cols:\n",
    "        if c not in tr.columns:\n",
    "            continue\n",
    "        tr_s = tr[c].astype(\"Int64\").astype(\"string\").fillna(\"__MISSING__\")\n",
    "        va_s = va[c].astype(\"Int64\").astype(\"string\").fillna(\"__MISSING__\")\n",
    "        top = (tr_s.value_counts(dropna=False).sort_values(ascending=False).head(max_levels).index.tolist())\n",
    "        kept[c] = top; keep = set(top)\n",
    "        tr_s = tr_s.where(tr_s.isin(keep), \"__OTHER__\")\n",
    "        va_s = va_s.where(va_s.isin(keep), \"__OTHER__\")\n",
    "        cats = list(dict.fromkeys(top + [\"__OTHER__\", \"__MISSING__\"]))\n",
    "        tr[c] = pd.Categorical(tr_s, categories=cats)\n",
    "        va[c] = pd.Categorical(va_s, categories=cats)\n",
    "    return tr, va, kept\n",
    "\n",
    "def one_hot_align(train: pd.DataFrame, valid: pd.DataFrame, cat_cols: List[str], drop_first=True):\n",
    "    tr = pd.get_dummies(train, columns=cat_cols, drop_first=drop_first)\n",
    "    va = pd.get_dummies(valid, columns=cat_cols, drop_first=drop_first)\n",
    "    for c in tr.columns:\n",
    "        if c not in va.columns:\n",
    "            va[c] = 0\n",
    "    va = va[tr.columns]\n",
    "    tr = tr.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    va = va.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    return tr, va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce579187-49e9-4410-ba12-a5b2f11267a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_count_glm(X: pd.DataFrame, y: np.ndarray, alpha: float = 0.1, max_iter: int = 10000, tol: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Ridge付きポアソン回帰（logリンク）。非負・安定なμを返すための学習器。\n",
    "    alpha は L2 正則化の強さ（デフォルト0.1でより柔軟に）。\n",
    "    \"\"\"\n",
    "    print(f\"          - fit_count_glm開始: X.shape={X.shape}, y.shape={y.shape}, alpha={alpha}, max_iter={max_iter}, tol={tol}\")\n",
    "    \n",
    "    # より柔軟なパラメーター設定\n",
    "    model = PoissonRegressor(\n",
    "        alpha=alpha, \n",
    "        fit_intercept=True, \n",
    "        max_iter=max_iter, \n",
    "        tol=tol,\n",
    "        warm_start=False,\n",
    "        solver='lbfgs'  # より安定したソルバー\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model.fit(np.asarray(X, float), y.astype(float))\n",
    "        print(f\"          - fit_count_glm完了: 収束={model.n_iter_}, 損失={model.score(np.asarray(X, float), y.astype(float)):.6f}\")\n",
    "        \n",
    "        # パラメーター情報を出力\n",
    "        print(f\"          - 学習パラメーター: alpha={alpha}, max_iter={max_iter}, tol={tol}\")\n",
    "        print(f\"          - 収束状況: {model.n_iter_} 回で収束\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"          - fit_count_glmエラー: {e}\")\n",
    "        # より緩いパラメーターで再試行\n",
    "        print(f\"          - 再試行: より緩いパラメーターで学習\")\n",
    "        model = PoissonRegressor(alpha=alpha*10, fit_intercept=True, max_iter=max_iter//2, tol=tol*10)\n",
    "        model.fit(np.asarray(X, float), y.astype(float))\n",
    "        print(f\"          - 再試行完了: 収束={model.n_iter_}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def poisson_grid_search(X: pd.DataFrame, y: np.ndarray, out_dir: Path) -> Tuple[Any, Dict[str, Any], float]:\n",
    "    \"\"\"Poissonパラメーターのグリッドサーチ\"\"\"\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import make_scorer, mean_squared_error\n",
    "    \n",
    "    print(f\"        - Poissonグリッドサーチ開始...\")\n",
    "    \n",
    "    # 小さい領域でのグリッドサーチ\n",
    "    param_grid = {\n",
    "        'alpha': [0.1,0.5,1.0],      # 正則化強度\n",
    "        'max_iter': [5000],      # 反復回数\n",
    "        'tol': [1e-6]             # 収束条件\n",
    "    }\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    results = []\n",
    "    \n",
    "    # グリッドサーチ実行\n",
    "    total_combinations = len(param_grid['alpha']) * len(param_grid['max_iter']) * len(param_grid['tol'])\n",
    "    print(f\"        - 総組み合わせ数: {total_combinations}\")\n",
    "    \n",
    "    for i, alpha in enumerate(param_grid['alpha']):\n",
    "        for j, max_iter in enumerate(param_grid['max_iter']):\n",
    "            for k, tol in enumerate(param_grid['tol']):\n",
    "                params = {'alpha': alpha, 'max_iter': max_iter, 'tol': tol}\n",
    "                print(f\"        - 試行 {i*4 + j*2 + k + 1}/{total_combinations}: alpha={alpha}, max_iter={max_iter}, tol={tol}\")\n",
    "                \n",
    "                try:\n",
    "                    # モデル学習\n",
    "                    model = fit_count_glm(X, y, **params)\n",
    "                    \n",
    "                    # 3-fold CVで評価（小さいデータセットなので3-fold）\n",
    "                    cv_scores = cross_val_score(\n",
    "                        model, X, y, \n",
    "                        cv=3, \n",
    "                        scoring=make_scorer(lambda y_true, y_pred: -mean_squared_error(y_true, y_pred)),\n",
    "                        n_jobs=1\n",
    "                    )\n",
    "                    mean_score = np.mean(cv_scores)\n",
    "                    \n",
    "                    print(f\"          - CVスコア: {mean_score:.6f} (±{np.std(cv_scores):.6f})\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'params': params,\n",
    "                        'cv_score': mean_score,\n",
    "                        'cv_std': np.std(cv_scores),\n",
    "                        'n_iter': model.n_iter_\n",
    "                    })\n",
    "                    \n",
    "                    # 最良スコアを更新\n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_params = params\n",
    "                        best_model = model\n",
    "                        print(f\"          - 最良スコア更新: {best_score:.6f}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"          - エラー: {e}\")\n",
    "                    results.append({\n",
    "                        'params': params,\n",
    "                        'cv_score': -np.inf,\n",
    "                        'cv_std': 0.0,\n",
    "                        'n_iter': 0,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "    \n",
    "    # 結果を保存\n",
    "    grid_results = {\n",
    "        'best_params': best_params,\n",
    "        'best_score': float(best_score),\n",
    "        'all_results': results,\n",
    "        'param_grid': param_grid\n",
    "    }\n",
    "    \n",
    "    # ディレクトリが存在することを確認してファイル保存\n",
    "    try:\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (out_dir / \"poisson_grid_search.json\").write_text(\n",
    "            json.dumps(grid_results, ensure_ascii=False, indent=2)\n",
    "        )\n",
    "        print(f\"        - グリッドサーチ結果を保存: {out_dir / 'poisson_grid_search.json'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"        - 警告: グリッドサーチ結果の保存に失敗: {e}\")\n",
    "        # ファイル保存に失敗しても処理は継続\n",
    "    \n",
    "    print(f\"        - グリッドサーチ完了: 最良パラメーター={best_params}, スコア={best_score:.6f}\")\n",
    "    \n",
    "    return best_model, best_params, best_score\n",
    "\n",
    "def glm_predict(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"PoissonRegressor の予測（平均 μ）。常に非負。\"\"\"\n",
    "    return np.asarray(model.predict(np.asarray(X, float)), float)\n",
    "\n",
    "def cap_mu(mu: np.ndarray, y_ref: np.ndarray, q: float = 0.999, mult: float = 2.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    予測の上限制御。訓練データの q 分位 × mult を上限にクリップ。\n",
    "    \"現実離れ\"な外れ値を物理的に封じる。\n",
    "    \"\"\"\n",
    "    cap = np.quantile(y_ref.astype(float), q) * mult\n",
    "    if not np.isfinite(cap) or cap <= 0:\n",
    "        return np.clip(mu, 0.0, None)\n",
    "    return np.clip(mu, 0.0, cap)\n",
    "\n",
    "# --- (参考) 元のNBモデル関数 ---\n",
    "def fit_nb(X: pd.DataFrame, y: np.ndarray):\n",
    "    X_np = np.asarray(X, dtype=float)\n",
    "    exog = sm.add_constant(X_np, has_constant=\"add\")\n",
    "    \n",
    "    # より安定した初期化\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=HessianInversionWarning)\n",
    "        try:\n",
    "            # より単純なPoissonモデルで初期化\n",
    "            pois = PoisDM(y, exog).fit(method=\"lbfgs\", maxiter=100, disp=False)\n",
    "            beta0 = pois.params\n",
    "        except Exception:\n",
    "            # より保守的な初期化\n",
    "            beta0 = np.zeros(exog.shape[1])\n",
    "            beta0[0] = np.log(np.mean(y) + 1e-6)  # 切片を平均の対数で初期化\n",
    "    \n",
    "    # より保守的な分散パラメータの初期化\n",
    "    start_params = np.r_[beta0, 0.5]  # 0.1 → 0.5\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=HessianInversionWarning)\n",
    "        model = NegativeBinomial(y, exog)\n",
    "        # より多くの反復回数とより安定した最適化\n",
    "        res = model.fit(method=\"lbfgs\", start_params=start_params, maxiter=1000, disp=False)\n",
    "    return model, res\n",
    "\n",
    "def nb_predict_with_se(res, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_np = np.asarray(X, dtype=float)\n",
    "    exog = sm.add_constant(X_np, has_constant=\"add\")\n",
    "    mu = np.asarray(res.predict(exog), float)\n",
    "    try:\n",
    "        cov = np.asarray(res.cov_params())\n",
    "        var_eta = np.einsum('ij,jk,ik->i', exog, cov, exog)\n",
    "        var_eta = np.clip(var_eta, 0.0, None)\n",
    "        se_mu = mu * np.sqrt(var_eta)  # Δ法\n",
    "    except Exception:\n",
    "        se_mu = np.full_like(mu, np.nan, dtype=float)\n",
    "    return mu, se_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78fddc-2888-44bb-979b-5cc0275e0da8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 最終モデルの学習用にPoaissonまたはNB回帰を行い、その結果を特徴量として導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db9ff4-1101-4c53-beb1-5629c2c816c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nb_features_for_val_from_train(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, target_col: str,\n",
    "    selected_num_cols: List[str], selected_cat_cols: List[str]\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"trainで学習したNBモデルでval用特徴量を生成（時系列リーク防止）\"\"\"\n",
    "    print(f\"    val用NB特徴量生成開始 (train学習済みモデル使用)\")\n",
    "    \n",
    "    use_feats = selected_num_cols + selected_cat_cols\n",
    "    sel_num = [c for c in use_feats if c in selected_num_cols]\n",
    "    sel_cat = [c for c in use_feats if c in selected_cat_cols]\n",
    "    print(f\"    - 使用特徴量: {len(sel_num)} 数値 + {len(sel_cat)} カテゴリ = {len(use_feats)} 総数\")\n",
    "\n",
    "    # trainデータの前処理\n",
    "    tr_df = df_train[sel_num + sel_cat].copy()\n",
    "    va_df = df_val[sel_num + sel_cat].copy()\n",
    "    \n",
    "    print(f\"    - trainデータ: {len(tr_df):,} 行\")\n",
    "    print(f\"    - valデータ: {len(va_df):,} 行\")\n",
    "\n",
    "    # 数値特徴量前処理（trainの統計で境界値を決定）\n",
    "    print(f\"    - 数値特徴量前処理中...\")\n",
    "    for c in sel_num:\n",
    "        tr_df[c] = tr_df[c].astype(float).fillna(0.0)\n",
    "        va_df[c] = va_df[c].astype(float).fillna(0.0)\n",
    "    \n",
    "    # NB用の前処理: 軽度のwinsorizationのみ（asinh scalingを削除）\n",
    "    Tr_cont, Va_cont, bounds = winsorize(tr_df[sel_num], va_df[sel_num], sel_num, 5.0/100, 95.0/100)\n",
    "    # asinh scalingを削除して、そのまま使用\n",
    "    improved_scalers = {}\n",
    "    tr_df[sel_num] = Tr_cont; va_df[sel_num] = Va_cont\n",
    "\n",
    "    # カテゴリ特徴量OHE変換（trainのレベルで固定）\n",
    "    print(f\"    - カテゴリ特徴量OHE変換中...\")\n",
    "    tr_cat_ready, va_cat_ready, _ = prepare_int_cats_for_ohe(tr_df, va_df, sel_cat, max_levels=30)\n",
    "    X_tr, X_va = one_hot_align(tr_cat_ready, va_cat_ready, sel_cat, drop_first=True)\n",
    "    print(f\"    - OHE後特徴量数: {X_tr.shape[1]}\")\n",
    "\n",
    "    # train全体でNB学習\n",
    "    y_tr = df_train[target_col].to_numpy()\n",
    "    print(f\"    - Poisson(Ridge)学習中...\")\n",
    "    model = fit_count_glm(X_tr, y_tr, alpha=1.0)\n",
    "\n",
    "    # val用予測（muのみ）\n",
    "    print(f\"    - val用予測中...\")\n",
    "    mu_va = glm_predict(model, X_va)\n",
    "    mu_va = cap_mu(mu_va, y_tr, q=0.999, mult=2.0)\n",
    "    \n",
    "    # val用NB予測のR²計算\n",
    "    y_val = df_val[target_col].to_numpy()\n",
    "    ss_res = np.sum((y_val - mu_va) ** 2)\n",
    "    ss_tot = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    r2 = 1.0 - (ss_res / (ss_tot + 1e-9))\n",
    "    \n",
    "    print(f\"    - val用NB予測完了: 平均予測値={np.mean(mu_va):.3f}, R²={r2:.6f}\")\n",
    "    print(f\"    - 実際値統計: 平均={np.mean(y_val):.3f}, 最小={np.min(y_val):.3f}, 最大={np.max(y_val):.3f}\")\n",
    "    print(f\"    - 予測値統計: 平均={np.mean(mu_va):.3f}, 最小={np.min(mu_va):.3f}, 最大={np.max(mu_va):.3f}\")\n",
    "\n",
    "    # val用特徴量を作成（muのみ）\n",
    "    nb_feat_val = pd.DataFrame({\"nb_mu\": mu_va})\n",
    "    \n",
    "    # 前処理情報を保存\n",
    "    preproc_info = {\n",
    "        \"winsor_bounds\": bounds,\n",
    "        \"improved_scalers\": improved_scalers,\n",
    "        \"sel_num\": sel_num,\n",
    "        \"sel_cat\": sel_cat,\n",
    "        \"ohe_columns\": list(X_tr.columns)\n",
    "    }\n",
    "    \n",
    "    return nb_feat_val, preproc_info\n",
    "\n",
    "def build_nb_oof_features(\n",
    "    df_train: pd.DataFrame, target_col: str,\n",
    "    top_rank: List[str], num_cols: List[str], cat_cols: List[str],\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]],\n",
    "    out_dir: Path, nb_top: int = NB_TOP\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"trainデータのみでNB OOF特徴量生成（時系列リーク防止）\"\"\"\n",
    "    print(f\"    Negative Binomial OOF特徴量生成開始 (trainのみ, 上位{nb_top}特徴量)\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    use_feats = top_rank[:nb_top]\n",
    "    sel_num = [c for c in use_feats if c in num_cols]\n",
    "    sel_cat = [c for c in use_feats if c in cat_cols]\n",
    "    print(f\"    - 使用特徴量: {len(sel_num)} 数値 + {len(sel_cat)} カテゴリ = {len(use_feats)} 総数\")\n",
    "    print(f\"    - trainデータ: {len(df_train):,} 行\")\n",
    "\n",
    "    oof_mu = np.full(len(df_train), np.nan, dtype=float)\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, start=1):\n",
    "        print(f\"      NB Fold {i}/{len(folds)} 処理中...\")\n",
    "        fdir = out_dir / f\"fold{i}\"\n",
    "        fdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # train内でのfold分割\n",
    "        tr_df = df_train.iloc[tr_idx][sel_num + sel_cat].copy()\n",
    "        va_df = df_train.iloc[va_idx][sel_num + sel_cat].copy()\n",
    "        print(f\"        - 学習データ: {len(tr_df):,} 行\")\n",
    "        print(f\"        - 検証データ: {len(va_df):,} 行\")\n",
    "\n",
    "        print(f\"        - 数値特徴量前処理中...\")\n",
    "        for c in sel_num:\n",
    "            tr_df[c] = tr_df[c].astype(float).fillna(0.0)\n",
    "            va_df[c] = va_df[c].astype(float).fillna(0.0)\n",
    "        # NB用の前処理: 軽度のwinsorizationのみ（asinh scalingを削除）\n",
    "        Tr_cont, Va_cont, bounds = winsorize(tr_df[sel_num], va_df[sel_num], sel_num, 5.0/100, 95.0/100)\n",
    "        # asinh scalingを削除して、そのまま使用\n",
    "        improved_scalers = {}\n",
    "        tr_df[sel_num] = Tr_cont; va_df[sel_num] = Va_cont\n",
    "\n",
    "        print(f\"        - カテゴリ特徴量OHE変換中...\")\n",
    "        tr_cat_ready, va_cat_ready, _ = prepare_int_cats_for_ohe(tr_df, va_df, sel_cat, max_levels=30)\n",
    "        X_tr, X_va = one_hot_align(tr_cat_ready, va_cat_ready, sel_cat, drop_first=True)\n",
    "        print(f\"        - OHE後特徴量数: {X_tr.shape[1]}\")\n",
    "\n",
    "        y_tr = y_train[tr_idx]; y_va = y_train[va_idx]\n",
    "        print(f\"        - Poisson(Ridge)学習中... (train: {len(y_tr):,} 行, val: {len(y_va):,} 行)\")\n",
    "        print(f\"        - y_tr統計: 平均={np.mean(y_tr):.3f}, 最小={np.min(y_tr):.3f}, 最大={np.max(y_tr):.3f}\")\n",
    "        model = fit_count_glm(X_tr, y_tr, alpha=1.0)\n",
    "        print(f\"        - Poisson学習完了\")\n",
    "        save_pickle(model, fdir / \"nb_model.pkl\")  # ファイル名は互換のまま\n",
    "        (fdir / \"numeric_preproc.json\").write_text(\n",
    "            json.dumps({\"winsor_bounds\": bounds, \"improved_scalers\": improved_scalers,\n",
    "                        \"sel_num\": sel_num, \"sel_cat\": sel_cat}, ensure_ascii=False, indent=2)\n",
    "        )\n",
    "\n",
    "        print(f\"        - 予測中...\")\n",
    "        mu_va = glm_predict(model, X_va)\n",
    "        mu_va = cap_mu(mu_va, y_tr, q=0.999, mult=2.0)\n",
    "        se_va = np.full_like(mu_va, np.nan, dtype=float)  # 互換用に列は残す\n",
    "\n",
    "        oof_mu[va_idx] = mu_va\n",
    "\n",
    "        # NB予測のR²計算\n",
    "        ss_res = np.sum((y_va - mu_va) ** 2)\n",
    "        ss_tot = np.sum((y_va - np.mean(y_va)) ** 2)\n",
    "        r2 = 1.0 - (ss_res / (ss_tot + 1e-9))\n",
    "\n",
    "        pd.DataFrame({\"y\": y_va, \"nb_mu\": mu_va, \"nb_se\": se_va}).to_csv(\n",
    "            fdir / \"nb_val_preds.csv\", index=False\n",
    "        )\n",
    "        print(f\"        - NB予測完了: 平均予測値={np.mean(mu_va):.3f}, R²={r2:.6f}\")\n",
    "        print(f\"        - 実際値統計: 平均={np.mean(y_va):.3f}, 最小={np.min(y_va):.3f}, 最大={np.max(y_va):.3f}\")\n",
    "        print(f\"        - 予測値統計: 平均={np.mean(mu_va):.3f}, 最小={np.min(mu_va):.3f}, 最大={np.max(mu_va):.3f}\")\n",
    "\n",
    "        print(f\"        - Fold {i} 処理完了\")\n",
    "        del tr_df, va_df, Tr_cont, Va_cont, X_tr, X_va, y_tr, y_va, model, mu_va, se_va\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"    NB OOF特徴量集約中...\")\n",
    "    nb_feat = pd.DataFrame({\"nb_mu\": oof_mu})\n",
    "    nb_feat.to_csv(out_dir / \"nb_oof_features.csv\", index=False)\n",
    "    print(f\"    - NB特徴量生成完了: {len(nb_feat.columns)} 特徴量\")\n",
    "\n",
    "    del oof_mu, y_train\n",
    "    gc.collect()\n",
    "    return nb_feat\n",
    "\n",
    "def build_poisson_features_full_data(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, target_col: str,\n",
    "    selected_num_cols: List[str], selected_cat_cols: List[str], out_dir: Path\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"trainデータのみでPoisson学習→train/val両方に推論適用\"\"\"\n",
    "    print(f\"    - trainデータのみでPoisson学習開始 (上位{len(selected_num_cols + selected_cat_cols)}特徴量)\")\n",
    "    \n",
    "    # trainデータのみで特徴量準備\n",
    "    tr_df = df_train[selected_num_cols + selected_cat_cols].copy()\n",
    "    va_df = df_val[selected_num_cols + selected_cat_cols].copy()\n",
    "    print(f\"    - 使用特徴量: {len(selected_num_cols)} 数値 + {len(selected_cat_cols)} カテゴリ\")\n",
    "    print(f\"    - trainデータ: {len(tr_df):,} 行, valデータ: {len(va_df):,} 行\")\n",
    "    \n",
    "    # 数値特徴量前処理（trainの統計で境界値を決定）\n",
    "    print(f\"    - 数値特徴量前処理中...\")\n",
    "    for c in selected_num_cols:\n",
    "        tr_df[c] = tr_df[c].astype(float).fillna(0.0)\n",
    "        va_df[c] = va_df[c].astype(float).fillna(0.0)\n",
    "    \n",
    "    # 軽度のwinsorization（trainの統計で境界値を決定）\n",
    "    Tr_cont, Va_cont, bounds = winsorize(tr_df[selected_num_cols], va_df[selected_num_cols], selected_num_cols, 5.0/100, 95.0/100)\n",
    "    \n",
    "    # 数値特徴量のスケーリング（trainの統計で標準化）\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    Tr_cont_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(Tr_cont), \n",
    "        columns=Tr_cont.columns, \n",
    "        index=Tr_cont.index\n",
    "    )\n",
    "    Va_cont_scaled = pd.DataFrame(\n",
    "        scaler.transform(Va_cont), \n",
    "        columns=Va_cont.columns, \n",
    "        index=Va_cont.index\n",
    "    )\n",
    "    \n",
    "    tr_df[selected_num_cols] = Tr_cont_scaled\n",
    "    va_df[selected_num_cols] = Va_cont_scaled\n",
    "    \n",
    "    # スケーリング後の統計を確認\n",
    "    print(f\"    - スケーリング後統計: train平均={np.mean(Tr_cont_scaled.values):.3f}, val平均={np.mean(Va_cont_scaled.values):.3f}\")\n",
    "    print(f\"    - スケーリング後範囲: train[{np.min(Tr_cont_scaled.values):.3f}, {np.max(Tr_cont_scaled.values):.3f}], val[{np.min(Va_cont_scaled.values):.3f}, {np.max(Va_cont_scaled.values):.3f}]\")\n",
    "    \n",
    "    # カテゴリ特徴量OHE変換（trainのレベルで固定）\n",
    "    print(f\"    - カテゴリ特徴量OHE変換中...\")\n",
    "    tr_cat_ready, va_cat_ready, _ = prepare_int_cats_for_ohe(tr_df, va_df, selected_cat_cols, max_levels=30)\n",
    "    X_tr, X_va = one_hot_align(tr_cat_ready, va_cat_ready, selected_cat_cols, drop_first=True)\n",
    "    print(f\"    - OHE後特徴量数: {X_tr.shape[1]}\")\n",
    "    \n",
    "    # trainデータのみでPoisson学習\n",
    "    y_tr = df_train[target_col].to_numpy()\n",
    "    print(f\"    - Poisson(Ridge)学習中... (trainのみ: {len(y_tr):,} 行)\")\n",
    "    print(f\"    - y統計: 平均={np.mean(y_tr):.3f}, 最小={np.min(y_tr):.3f}, 最大={np.max(y_tr):.3f}\")\n",
    "    \n",
    "    # パラメーターグリッドサーチ\n",
    "    model, best_params, best_score = poisson_grid_search(X_tr, y_tr, out_dir)\n",
    "    print(f\"    - Poisson学習完了 (最良スコア: {best_score:.6f})\")\n",
    "    \n",
    "    # モデルとパラメーターを保存\n",
    "    save_pickle(model, out_dir / \"poisson_model.pkl\")\n",
    "    \n",
    "    # パラメーター情報を保存\n",
    "    model_info = {\n",
    "        \"poisson_params\": best_params,\n",
    "        \"best_score\": float(best_score),\n",
    "        \"n_iter\": int(model.n_iter_),\n",
    "        \"n_features\": int(X_tr.shape[1]),\n",
    "        \"train_size\": int(len(y_tr)),\n",
    "        \"converged\": bool(model.n_iter_ < best_params[\"max_iter\"])\n",
    "    }\n",
    "    try:\n",
    "        (out_dir / \"poisson_model_info.json\").write_text(\n",
    "            json.dumps(model_info, ensure_ascii=False, indent=2)\n",
    "        )\n",
    "        print(f\"    - モデル情報を保存: {out_dir / 'poisson_model_info.json'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    - 警告: モデル情報の保存に失敗: {e}\")\n",
    "    \n",
    "    # train/val両方に推論適用\n",
    "    print(f\"    - train予測中... ({len(X_tr):,} 行)\")\n",
    "    mu_train = glm_predict(model, X_tr)\n",
    "    mu_train = cap_mu(mu_train, y_tr, q=0.999, mult=2.0)\n",
    "    \n",
    "    print(f\"    - val予測中... ({len(X_va):,} 行)\")\n",
    "    mu_val = glm_predict(model, X_va)\n",
    "    mu_val = cap_mu(mu_val, y_tr, q=0.999, mult=2.0)  # trainの統計でクリップ\n",
    "    \n",
    "    # 特徴量作成\n",
    "    nb_feats_train = pd.DataFrame({\"nb_mu\": mu_train})\n",
    "    nb_feats_val = pd.DataFrame({\"nb_mu\": mu_val})\n",
    "    \n",
    "    # R²計算\n",
    "    y_val = df_val[target_col].to_numpy()\n",
    "    \n",
    "    ss_res_train = np.sum((y_tr - mu_train) ** 2)\n",
    "    ss_tot_train = np.sum((y_tr - np.mean(y_tr)) ** 2)\n",
    "    r2_train = 1.0 - (ss_res_train / (ss_tot_train + 1e-9))\n",
    "    \n",
    "    ss_res_val = np.sum((y_val - mu_val) ** 2)\n",
    "    ss_tot_val = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    r2_val = 1.0 - (ss_res_val / (ss_tot_val + 1e-9))\n",
    "    \n",
    "    print(f\"    - train R²: {r2_train:.6f}, val R²: {r2_val:.6f}\")\n",
    "    print(f\"    - train予測統計: 平均={np.mean(mu_train):.3f}, 最小={np.min(mu_train):.3f}, 最大={np.max(mu_train):.3f}\")\n",
    "    print(f\"    - val予測統計: 平均={np.mean(mu_val):.3f}, 最小={np.min(mu_val):.3f}, 最大={np.max(mu_val):.3f}\")\n",
    "    \n",
    "    # 予測値の多様性を確認\n",
    "    train_unique = len(np.unique(mu_train))\n",
    "    val_unique = len(np.unique(mu_val))\n",
    "    print(f\"    - 予測値多様性: train={train_unique:,} 種類, val={val_unique:,} 種類\")\n",
    "    \n",
    "    if train_unique < 10 or val_unique < 10:\n",
    "        print(f\"    警告: 予測値の多様性が低い可能性があります\")\n",
    "        print(f\"    - パラメーター調整を検討してください: alpha={best_params['alpha']}\")\n",
    "    \n",
    "    # 前処理情報\n",
    "    preproc_info = {\n",
    "        \"winsor_bounds\": bounds,\n",
    "        \"scaler_mean\": scaler.mean_.tolist(),\n",
    "        \"scaler_scale\": scaler.scale_.tolist(),\n",
    "        \"selected_num\": selected_num_cols,\n",
    "        \"selected_cat\": selected_cat_cols,\n",
    "        \"ohe_columns\": list(X_tr.columns),\n",
    "        \"poisson_params\": best_params,\n",
    "        \"model_info\": model_info,\n",
    "        \"r2_train\": float(r2_train),\n",
    "        \"r2_val\": float(r2_val)\n",
    "    }\n",
    "    \n",
    "    return nb_feats_train, nb_feats_val, preproc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b02f03-caaa-4015-b786-05fb0082fe95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LightGBMの最終(第3ステップ)学習\n",
    "- Step 3 : Step 2で選んだK個の特徴量にPoaissonまたはNB回帰の出力値を加えたK+1個の特徴量を用いてLightGBMを学習\n",
    "           (ただし、CVはせずに、データは全期間データを8:2(train/val)に分割して学習)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311db3aa-644d-4585-a2f4-2a651e1a5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_with_nbfeatures_fulltrain(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, target_col: str,\n",
    "    avg_importance: pd.DataFrame,\n",
    "    nb_feats_train: pd.DataFrame, nb_feats_val: pd.DataFrame,\n",
    "    num_cols: List[str], cat_cols: List[str],\n",
    "    best_K: int, out_dir: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"train学習→val予測の最終LightGBM（時系列リーク防止）\"\"\"\n",
    "    print(f\"    最終LightGBM学習開始 (K={best_K})\")\n",
    "    rank = avg_importance[\"feature\"].tolist()\n",
    "    base_feats = rank[:best_K]\n",
    "    base_num = [c for c in base_feats if c in num_cols]\n",
    "    base_cat = [c for c in base_feats if c in cat_cols]\n",
    "    print(f\"    - ベース特徴量: {len(base_num)} 数値 + {len(base_cat)} カテゴリ\")\n",
    "\n",
    "    final_num = base_num + [\"nb_mu\"]\n",
    "    final_cat = base_cat\n",
    "    print(f\"    - 最終特徴量: {len(final_num)} 数値 + {len(final_cat)} カテゴリ = {len(final_num + final_cat)} 総数\")\n",
    "\n",
    "    kdir = out_dir / f\"final_full_with_nb_K{best_K}\"\n",
    "    kdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # trainデータでの学習\n",
    "    print(f\"    - trainデータ準備中...\")\n",
    "    df_train_aug = df_train[base_num + base_cat].copy().reset_index(drop=True)\n",
    "    df_train_aug = pd.concat([df_train_aug, nb_feats_train.reset_index(drop=True)], axis=1)\n",
    "    print(f\"    - train結合後特徴量数: {len(df_train_aug.columns)}\")\n",
    "\n",
    "    # valデータの準備\n",
    "    print(f\"    - valデータ準備中...\")\n",
    "    df_val_aug = df_val[base_num + base_cat].copy().reset_index(drop=True)\n",
    "    df_val_aug = pd.concat([df_val_aug, nb_feats_val.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"    - train前処理中...\")\n",
    "    X_train = lgb_preprocess_for_lgb(df_train_aug, final_num, final_cat)\n",
    "    y_train = df_train[target_col].to_numpy()\n",
    "    print(f\"    - train学習データ: {len(X_train):,} 行 × {X_train.shape[1]} 特徴量\")\n",
    "\n",
    "    print(f\"    - val前処理中...\")\n",
    "    X_val = lgb_preprocess_for_lgb(df_val_aug, final_num, final_cat)\n",
    "    y_val = df_val[target_col].to_numpy()\n",
    "    print(f\"    - val予測データ: {len(X_val):,} 行 × {X_val.shape[1]} 特徴量\")\n",
    "\n",
    "    print(f\"    - train学習中...\")\n",
    "    booster_full, _, _ = lgb_train_eval(X_train, y_train, X_val, y_val, final_cat)\n",
    "    save_pickle(booster_full, kdir / \"lgb_model_full.pkl\")\n",
    "\n",
    "    print(f\"    - val予測中...\")\n",
    "    pred_val = booster_full.predict(X_val, num_iteration=booster_full.best_iteration)\n",
    "    \n",
    "    # 予測結果を保存\n",
    "    pd.DataFrame({\"y_val\": y_val, \"yhat_val\": pred_val}).to_csv(\n",
    "        kdir / \"val_predictions.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # 最終学習のR²計算\n",
    "    ss_res = np.sum((y_val - pred_val) ** 2)\n",
    "    ss_tot = np.sum((y_val - np.mean(y_val)) ** 2)\n",
    "    val_r2 = 1.0 - (ss_res / (ss_tot + 1e-9))\n",
    "    \n",
    "    summary = {\n",
    "        \"final_features\": {\"numeric\": final_num, \"categorical\": final_cat},\n",
    "        \"n_train\": int(len(X_train)),\n",
    "        \"n_val\": int(len(X_val)),\n",
    "        \"best_K\": int(best_K),\n",
    "        \"train_only\": True,\n",
    "        \"val_rmse\": float(np.sqrt(np.mean((y_val - pred_val) ** 2))),\n",
    "        \"val_r2\": val_r2\n",
    "    }\n",
    "    (kdir / \"summary.json\").write_text(json.dumps(summary, ensure_ascii=False, indent=2))\n",
    "    print(f\"    - 最終モデル学習完了: train={len(X_train):,}, val={len(X_val):,}, val_RMSE={summary['val_rmse']:.6f}, val_R²={summary['val_r2']:.6f}\")\n",
    "\n",
    "    del df_train_aug, df_val_aug, X_train, X_val, y_train, y_val, pred_val, booster_full\n",
    "    gc.collect()\n",
    "\n",
    "    return {\"summary\": summary, \"model_path\": str((kdir / \"lgb_model_full.pkl\").resolve())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f060a-c48a-4b92-a0cd-9a1859b48656",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 学習パイプラインの設定(上記までの関数の流れを設定)と評価関数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be46e0-6899-4ced-b742-e751768a613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task_df_and_splitinfo(task: str) -> Tuple[pd.DataFrame, str, int, int, pd.Timestamp]:\n",
    "    \"\"\"データ読み込みと時系列分割情報を返す\"\"\"\n",
    "    df, target = load_src_df(task)\n",
    "    cutoff_date, tr_idx, va_idx = time_split_by_date(df, train_ratio=0.8, date_col=DATE_COL)\n",
    "    n_train, n_val = len(tr_idx), len(va_idx)\n",
    "    return df, target, n_train, n_val, cutoff_date\n",
    "\n",
    "def get_train_val_split(df_all: pd.DataFrame, cutoff_date: pd.Timestamp) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"全データからtrain/valを分離（時系列リーク防止）\"\"\"\n",
    "    train_mask = df_all[DATE_COL] <= cutoff_date\n",
    "    val_mask = df_all[DATE_COL] > cutoff_date\n",
    "    \n",
    "    df_train = df_all[train_mask].copy().reset_index(drop=True)\n",
    "    df_val = df_all[val_mask].copy().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"        - train/val分離: train={len(df_train):,} 行, val={len(df_val):,} 行\")\n",
    "    return df_train, df_val\n",
    "\n",
    "def run_pipeline_for_task(task: str):\n",
    "    print(f\"  タスク '{task}' の出力ディレクトリを作成中...\")\n",
    "    out_dir = ROOT_OUT_DIR / task\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\" データロード & 時系列分割中...\")\n",
    "    df_all, target_col, n_train, n_val, cutoff_date = load_task_df_and_splitinfo(task)\n",
    "    print(f\"    - 全データ数: {len(df_all):,}\")\n",
    "    print(f\"    - 目的変数: {target_col}\")\n",
    "    print(f\"    - 学習データ数: {n_train:,} (80%)\")\n",
    "    print(f\"    - 検証データ数: {n_val:,} (20%)\")\n",
    "    print(f\"    - 分割基準日: {cutoff_date}\")\n",
    "\n",
    "    (out_dir / \"basic_info.json\").write_text(json.dumps({\n",
    "        \"task\": task, \"target\": target_col,\n",
    "        \"n_all\": int(len(df_all)), \"n_train_time80\": n_train, \"n_val_time20\": n_val,\n",
    "        \"cutoff_date\": str(cutoff_date)\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print(f\"  特徴量選択中...\")\n",
    "    num_all, cat_all = pick_feature_cols(df_all, target_col)\n",
    "    print(f\"    - 数値特徴量数: {len(num_all)}\")\n",
    "    print(f\"    - カテゴリ特徴量数: {len(cat_all)}\")\n",
    "\n",
    "    # train/val分離\n",
    "    df_train, df_val = get_train_val_split(df_all, cutoff_date)\n",
    "    \n",
    "    print(f\"  3-fold CV設定中...\")\n",
    "    # train内での3-fold CV（sold_dateベース）\n",
    "    folds_train = make_3fold_80_20_indices(df_train, date_col=DATE_COL)\n",
    "    (out_dir / \"folds.json\").write_text(json.dumps(\n",
    "        [{\"fold\": i+1, \"n_train\": int(len(tr)), \"n_val\": int(len(va))} for i, (tr, va) in enumerate(folds_train)],\n",
    "        ensure_ascii=False, indent=2)\n",
    "    )\n",
    "    print(f\"    - Fold設定完了: {len(folds_train)} folds\")\n",
    "\n",
    "    print(f\"  Step 1/4: LightGBM特徴重要度計算中...\")\n",
    "    imp_dir = out_dir / \"importance_cv\"\n",
    "    avg_imp = lgb_fold_importances_raw(df_train, target_col, num_all, cat_all, folds_train, imp_dir)\n",
    "    print(f\"    - 重要度計算完了: {len(avg_imp)} 特徴量\")\n",
    "\n",
    "    print(f\"  Step 2/4: KグリッドCV実行中...\")\n",
    "    kcv_dir = out_dir / \"kgrid_lgb_cv\"\n",
    "    kres = lgb_kgrid_cv_raw(df_train, target_col, num_all, cat_all, avg_imp, folds_train, K_CANDIDATES, kcv_dir)\n",
    "    best_row = kres.iloc[0].to_dict()\n",
    "    best_K = int(best_row[\"K\"])\n",
    "    (out_dir / \"best_k.json\").write_text(json.dumps(best_row, ensure_ascii=False, indent=2))\n",
    "    print(f\"    - 最良K: {best_K}\")\n",
    "    print(f\"    - 最良RMSE: {best_row['RMSE_va_mean']:.6f}\")\n",
    "\n",
    "    print(f\"  Step 3/4: Poisson特徴量生成中...\")\n",
    "    nb_dir = out_dir / \"nb_oof\"\n",
    "    \n",
    "    # 全データ結合でPoissonモデル学習\n",
    "    selected_features = avg_imp[\"feature\"].tolist()[:NB_TOP]\n",
    "    selected_num = [c for c in selected_features if c in num_all]\n",
    "    selected_cat = [c for c in selected_features if c in cat_all]\n",
    "    \n",
    "    nb_feats_train, nb_feats_val, nb_preproc_info = build_poisson_features_full_data(\n",
    "        df_train, df_val, target_col, selected_num, selected_cat, nb_dir\n",
    "    )\n",
    "    print(f\"    - train Poisson特徴量生成完了: {len(nb_feats_train.columns)} 特徴量\")\n",
    "    print(f\"    - val Poisson特徴量生成完了: {len(nb_feats_val.columns)} 特徴量\")\n",
    "    \n",
    "    # 前処理情報を保存\n",
    "    (nb_dir / \"preproc_info.json\").write_text(\n",
    "        json.dumps(nb_preproc_info, ensure_ascii=False, indent=2)\n",
    "    )\n",
    "\n",
    "    print(f\"  Step 4/4: 最終LightGBM学習中...\")\n",
    "    final = lgb_with_nbfeatures_fulltrain(df_train, df_val, target_col, avg_imp, nb_feats_train, nb_feats_val, num_all, cat_all, best_K, out_dir)\n",
    "    (out_dir / \"final_summary.json\").write_text(json.dumps(final, ensure_ascii=False, indent=2))\n",
    "    print(f\"    - 最終モデル学習完了\")\n",
    "\n",
    "    (out_dir / \"time_split.json\").write_text(json.dumps({\n",
    "        \"n_train\": n_train, \"n_val\": n_val, \"cutoff_date\": str(cutoff_date)\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print(f\"  タスク '{task}' 完了: 最良K={best_K}, RMSE_va_mean={best_row['RMSE_va_mean']:.6f}\")\n",
    "    print(f\"  出力先: {out_dir.resolve()}\")\n",
    "\n",
    "    #学習ステップごとにメモリを解放する\n",
    "    print(f\"  メモリ解放中...\")\n",
    "    to_delete = [df_all, df_train, df_val, folds_train, avg_imp, kres, best_row, nb_feats_train, nb_feats_val, final, num_all, cat_all]\n",
    "    for obj in to_delete:\n",
    "        try: del obj\n",
    "        except Exception: pass\n",
    "    gc.collect()\n",
    "    print(f\"  メモリ解放完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70993d5-28d4-458c-8f3b-1a45795b0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_final_feature_spec(task_dir: Path) -> Tuple[List[str], List[str], str]:\n",
    "    cand = sorted(task_dir.glob(\"final_full_with_nb_K*/summary.json\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No final summary.json under {task_dir}\")\n",
    "    summ_path = cand[0]\n",
    "    with open(summ_path, \"r\") as f:\n",
    "        summ = json.load(f)\n",
    "    feats = summ[\"final_features\"]\n",
    "    final_num = feats[\"numeric\"]\n",
    "    final_cat = feats[\"categorical\"]\n",
    "    model_path = str(summ_path.parent.joinpath(\"lgb_model_full.pkl\"))\n",
    "    return final_num, final_cat, model_path\n",
    "\n",
    "def load_time_split_counts(task: str) -> Tuple[int, int, pd.Timestamp]:\n",
    "    \"\"\"学習時の時系列分割情報を読み込み\"\"\"\n",
    "    task_dir = ROOT_OUT_DIR / task\n",
    "    sp = task_dir / \"time_split.json\"\n",
    "    if sp.exists():\n",
    "        js = json.loads(sp.read_text())\n",
    "        return int(js[\"n_train\"]), int(js[\"n_val\"]), pd.to_datetime(js[\"cutoff_date\"])\n",
    "    df, _ = load_src_df(task)\n",
    "    cutoff, tr, va = time_split_by_date(df, 0.8, DATE_COL)\n",
    "    return len(tr), len(va), cutoff\n",
    "\n",
    "def build_X_for_inference(df_base: pd.DataFrame, nb_val: pd.DataFrame,\n",
    "                          final_num: List[str], final_cat: List[str]) -> pd.DataFrame:\n",
    "    base_num = [c for c in final_num if c not in (\"nb_mu\")]\n",
    "    X_num = df_base.reindex(columns=base_num, fill_value=0.0).copy()\n",
    "    nb_cols = [\"nb_mu\"]\n",
    "    for c in nb_cols:\n",
    "        if c not in nb_val.columns:\n",
    "            raise KeyError(f\"nb features missing: {c}\")\n",
    "    X_nb = nb_val[nb_cols].copy()\n",
    "    X_cat = df_base.reindex(columns=final_cat, fill_value=-1).copy()\n",
    "    X_all = pd.concat([X_num, X_nb, X_cat], axis=1)\n",
    "    X_all = lgb_preprocess_for_lgb(X_all, final_num, final_cat)\n",
    "    assert list(X_all.columns) == (final_num + final_cat), \"Column order mismatch\"\n",
    "    return X_all\n",
    "\n",
    "def load_nb_features_for_val(task_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"val用NB特徴量を読み込み\"\"\"\n",
    "    # val用NB特徴量を直接読み込み\n",
    "    val_pred_path = task_dir / \"final_full_with_nb_K*\" / \"val_predictions.csv\"\n",
    "    val_pred_candidates = list(task_dir.glob(\"final_full_with_nb_K*/val_predictions.csv\"))\n",
    "    \n",
    "    if not val_pred_candidates:\n",
    "        raise FileNotFoundError(f\"No val predictions found in {task_dir}\")\n",
    "    \n",
    "    val_pred_path = val_pred_candidates[0]\n",
    "    val_pred_df = pd.read_csv(val_pred_path)\n",
    "    \n",
    "    # NB特徴量を再構築（val予測から）\n",
    "    nb_val = pd.DataFrame({\"nb_mu\": val_pred_df[\"yhat_val\"]})\n",
    "    return nb_val\n",
    "\n",
    "def evaluate_single_task_val(task: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"単一タスクの検証データでの評価（時系列リーク防止）\"\"\"\n",
    "    # 学習時と同じ分割でvalデータを取得\n",
    "    df_all, target_col = load_src_df(task)\n",
    "    _, _, cutoff_date = load_time_split_counts(task)\n",
    "    df_all = df_all.sort_values(DATE_COL).reset_index(drop=True)\n",
    "    val_mask = df_all[DATE_COL] > cutoff_date\n",
    "    y_val = df_all.loc[val_mask, target_col].to_numpy()\n",
    "\n",
    "    task_dir = ROOT_OUT_DIR / task\n",
    "    final_num, final_cat, model_path = load_final_feature_spec(task_dir)\n",
    "\n",
    "    # val用NB特徴量を読み込み\n",
    "    nb_val = load_nb_features_for_val(task_dir)\n",
    "\n",
    "    # ベース特徴量を準備\n",
    "    use_base_cols = [c for c in (final_num + final_cat) if c not in (\"nb_mu\")]\n",
    "    df_val_base = df_all.loc[val_mask, use_base_cols].reindex(columns=use_base_cols, fill_value=0).reset_index(drop=True)\n",
    "\n",
    "    # 推論用特徴量を構築\n",
    "    X_val = build_X_for_inference(df_val_base, nb_val, final_num, final_cat)\n",
    "\n",
    "    # 学習済みモデルで予測\n",
    "    booster: lgb.Booster = pickle.load(open(model_path, \"rb\"))\n",
    "    yhat = booster.predict(X_val, num_iteration=booster.best_iteration)\n",
    "\n",
    "    # 評価結果を保存\n",
    "    (task_dir / \"eval\").mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame({\"y_val\": y_val, \"yhat\": yhat}).to_csv(task_dir / \"eval\" / \"val_predictions.csv\", index=False)\n",
    "\n",
    "    return y_val, yhat\n",
    "\n",
    "def evaluate_sum_task_fixed():\n",
    "    y_val, yhat = evaluate_single_task_val(\"sum\")\n",
    "    return y_val, yhat, None\n",
    "\n",
    "def evaluate_horizon_sum_vs_sumval_fixed():\n",
    "    \"\"\"h1-h26の合算とsumの検証データを比較\"\"\"\n",
    "    df_sum, _ = load_src_df(\"sum\")\n",
    "    _, _, cutoff_date = load_time_split_counts(\"sum\")\n",
    "    df_sum = df_sum.sort_values(DATE_COL).reset_index(drop=True)\n",
    "    y_true_sum = df_sum.loc[df_sum[DATE_COL] > cutoff_date, SUM_TARGET].to_numpy()\n",
    "\n",
    "    preds = []\n",
    "    for N in range(1, 27):\n",
    "        task = f\"h{str(N).zfill(2)}\"\n",
    "        _, yhat_h = evaluate_single_task_val(task)\n",
    "        preds.append(yhat_h)\n",
    "\n",
    "    yhat_sum = np.sum(np.vstack(preds), axis=0)\n",
    "    return y_true_sum, yhat_sum\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"評価指標を計算\"\"\"\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    eps = 1e-9\n",
    "    rmse = float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
    "    mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "    smape = float(np.mean(2.0 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + eps)))\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2) + eps)\n",
    "    r2 = float(1.0 - ss_res / ss_tot)\n",
    "    bias = float(np.mean(y_pred - y_true))\n",
    "    return {\n",
    "        \"n_val\": int(y_true.size),\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"smape\": smape,\n",
    "        \"r2\": r2,\n",
    "        \"bias_mean\": bias,\n",
    "        \"sum_true\": float(np.sum(y_true)),\n",
    "        \"sum_pred\": float(np.sum(y_pred)),\n",
    "        \"sum_diff\": float(np.sum(y_pred) - np.sum(y_true)),\n",
    "    }\n",
    "\n",
    "def run_all_evaluations_and_write():\n",
    "    print(\"  🔍 全タスク評価開始\")\n",
    "    rows = []\n",
    "\n",
    "    tasks = [\"sum\"] + [f\"h{str(n).zfill(2)}\" for n in range(1, 27)]\n",
    "    print(f\"  評価対象タスク: {len(tasks)} 個\")\n",
    "    \n",
    "    for i, task in enumerate(tasks, 1):\n",
    "        print(f\"    [{i:2d}/27] タスク '{task}' 評価中...\")\n",
    "        task_dir = ROOT_OUT_DIR / task\n",
    "        try:\n",
    "            if not (task_dir.exists() and list(task_dir.glob(\"final_full_with_nb_K*/summary.json\"))):\n",
    "                print(f\"      スキップ: 最終モデルが見つかりません\")\n",
    "                continue\n",
    "            y_val, yhat = evaluate_single_task_val(task)\n",
    "            m = compute_metrics(y_val, yhat)\n",
    "            m[\"task\"] = task\n",
    "            rows.append(m)\n",
    "            print(f\"      完了: RMSE={m['rmse']:.6f}  MAE={m['mae']:.6f}  sMAPE={m['smape']:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      エラー: {e}\")\n",
    "\n",
    "    print(f\"  合算評価実行中...\")\n",
    "    try:\n",
    "        y_true_sum, yhat_sum = evaluate_horizon_sum_vs_sumval_fixed()\n",
    "        m_sum = compute_metrics(y_true_sum, yhat_sum)\n",
    "        m_sum[\"task\"] = \"h_sum_vs_sum_val\"\n",
    "        rows.append(m_sum)\n",
    "        print(f\"  合算評価完了: RMSE={m_sum['rmse']:.6f}  MAE={m_sum['mae']:.6f}  sMAPE={m_sum['smape']:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  合算評価エラー: {e}\")\n",
    "\n",
    "    if rows:\n",
    "        print(f\"  評価結果集約中...\")\n",
    "        df_metrics = pd.DataFrame(rows)[[\n",
    "            \"task\",\"n_val\",\"rmse\",\"mae\",\"smape\",\"r2\",\"bias_mean\",\"sum_true\",\"sum_pred\",\"sum_diff\"\n",
    "        ]].sort_values(\"task\").reset_index(drop=True)\n",
    "        df_metrics.to_csv(EVAL_CSV_PATH, index=False)\n",
    "        with open(EVAL_JSON_PATH, \"w\") as f:\n",
    "            json.dump({\"metrics\": rows}, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  集約CSV: {EVAL_CSV_PATH.resolve()}\")\n",
    "        print(f\"  JSON:     {EVAL_JSON_PATH.resolve()}\")\n",
    "        print(f\"  評価完了: {len(rows)} タスク\")\n",
    "    else:\n",
    "        print(\"  出力対象がありません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89c330-e624-4861-bdbb-a5f2f510c2eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# パイプライン実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c23176-f434-49de-be0d-bea2a8885a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"時系列リーク防止機械学習パイプライン開始\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"出力ディレクトリ: {ROOT_OUT_DIR}\")\n",
    "    print(f\"処理対象タスク数: 27 (sum + h1-h26)\")\n",
    "    print(f\"目的変数ベース: {TARGET_BASE}\")\n",
    "    print(f\"LightGBM設定: {LGB_NUM_BOOST_ROUND}回学習, early_stopping={LGB_EARLY_STOP_ROUNDS}\")\n",
    "    print(f\"K候補: {K_CANDIDATES}\")\n",
    "    print(f\"NB上位特徴数: {NB_TOP}\")\n",
    "    print(\"時系列リーク防止: train/val厳密分割, 同一日付跨ぎなし\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=HessianInversionWarning)\n",
    "\n",
    "    tasks = [\"sum\"] + [f\"h{str(n).zfill(2)}\" for n in range(1, 27)]\n",
    "    print(f\"処理タスク一覧: {tasks}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, task in enumerate(tasks, 1):\n",
    "        print(f\"\\n[{i:2d}/27] タスク '{task}' 開始...\")\n",
    "        try:\n",
    "            run_pipeline_for_task(task)\n",
    "            print(f\"[{i:2d}/27] タスク '{task}' 完了\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i:2d}/27] タスク '{task}' エラー: {e}\")\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"学習フェーズ完了\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"成果物ルート: {ROOT_OUT_DIR.resolve()}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"検証フェーズ開始\")\n",
    "    print(\"=\" * 80)\n",
    "    run_all_evaluations_and_write()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"全タスク・合算の検証完了\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"検証CSV: {EVAL_CSV_PATH.resolve()}\")\n",
    "# --- 警告を非表示に設定 ---\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=HessianInversionWarning)\n",
    "\n",
    "# --- パイプライン実行 ---\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
